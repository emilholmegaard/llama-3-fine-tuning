{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Fine-Tuning of Llama 3.3 with LoRA\n",
    "\n",
    "This notebook demonstrates how to fine-tune a Llama 3.3 model using Low-Rank Adaptation (LoRA). LoRA is a parameter-efficient fine-tuning technique that significantly reduces memory requirements while maintaining performance.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "1. Installed all required dependencies (see `requirements.txt`)\n",
    "2. Access to a GPU with at least 16GB VRAM (for 8B model)\n",
    "3. Prepared your training dataset in the appropriate format\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Add the repository root to the path so we can import modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Fine-tuning will be extremely slow on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Let's define our fine-tuning configuration. You can adjust these parameters based on your specific requirements and hardware constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define configuration\n",
    "config = {\n",
    "    \"model\": {\n",
    "        \"base_model\": \"meta-llama/Llama-3.3-8B\",\n",
    "        \"output_dir\": \"../data/models/llama-3-lora-basic/\",\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 4,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"warmup_ratio\": 0.05,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr_scheduler_type\": \"cosine\",\n",
    "        \"bf16\": True,  # Use bfloat16 for mixed precision training\n",
    "        \"gradient_checkpointing\": True,  # Enable gradient checkpointing to save memory\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"train_file\": \"../data/processed/dataset/train.jsonl\",\n",
    "        \"validation_file\": \"../data/processed/dataset/val.jsonl\",\n",
    "        \"max_seq_length\": 2048,\n",
    "    },\n",
    "    \"lora\": {\n",
    "        \"r\": 16,  # LoRA attention dimension\n",
    "        \"alpha\": 32,  # LoRA alpha parameter\n",
    "        \"dropout\": 0.05,\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention modules for 8B model\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(config[\"model\"][\"output_dir\"], exist_ok=True)\n",
    "\n",
    "# Save configuration to output directory for reproducibility\n",
    "with open(os.path.join(config[\"model\"][\"output_dir\"], \"config.yaml\"), \"w\") as f:\n",
    "    yaml.dump(config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "Load and prepare the dataset for fine-tuning. We expect datasets in JSONL format with an instruction format structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if data files exist\n",
    "train_file = config[\"data\"][\"train_file\"]\n",
    "validation_file = config[\"data\"][\"validation_file\"]\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    raise FileNotFoundError(f\"Training file not found: {train_file}\")\n",
    "if not os.path.exists(validation_file):\n",
    "    raise FileNotFoundError(f\"Validation file not found: {validation_file}\")\n",
    "\n",
    "print(f\"Loading dataset from {train_file} and {validation_file}\")\n",
    "\n",
    "# Load datasets\n",
    "dataset = load_dataset('json', data_files={\n",
    "    'train': train_file,\n",
    "    'validation': validation_file\n",
    "})\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"Dataset format: {dataset}\")\n",
    "print(f\"Training examples: {len(dataset['train'])}\")\n",
    "print(f\"Validation examples: {len(dataset['validation'])}\")\n",
    "print(\"\\nSample data:\")\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the Data\n",
    "\n",
    "For instruction fine-tuning, we need to format our data in a standardized way that the model can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config[\"model\"][\"base_model\"],\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Set padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define prompt template for instruction tuning\n",
    "PROMPT_TEMPLATE = \"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{output}\n",
    "\"\"\"\n",
    "\n",
    "# Function to format examples\n",
    "def format_instruction(example):\n",
    "    instruction = example.get(\"instruction\", \"\")\n",
    "    input_text = example.get(\"input\", \"\")\n",
    "    output = example.get(\"output\", \"\")\n",
    "    \n",
    "    # Format according to the template\n",
    "    text = PROMPT_TEMPLATE.format(\n",
    "        instruction=instruction,\n",
    "        input=input_text,\n",
    "        output=output\n",
    "    )\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "# Apply formatting to dataset\n",
    "formatted_dataset = dataset.map(format_instruction, remove_columns=dataset[\"train\"].column_names)\n",
    "print(\"\\nFormatted sample:\")\n",
    "print(formatted_dataset[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the Dataset\n",
    "\n",
    "Now we'll tokenize our formatted dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to tokenize examples\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=config[\"data\"][\"max_seq_length\"]\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# Tokenize dataset\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "# Display tokenized dataset info\n",
    "print(f\"Tokenized dataset: {tokenized_dataset}\")\n",
    "print(f\"Features: {tokenized_dataset['train'].features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Preparation\n",
    "\n",
    "Load the base model and prepare it for LoRA fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load base model\n",
    "print(f\"Loading base model: {config['model']['base_model']}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config[\"model\"][\"base_model\"],\n",
    "    torch_dtype=torch.bfloat16 if config[\"training\"][\"bf16\"] else torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Display model size\n",
    "total_params = sum(p.numel() for p in model.parameters()) / 1e9\n",
    "print(f\"Model loaded with {total_params:.2f} billion parameters\")\n",
    "\n",
    "# Set up LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=config[\"lora\"][\"r\"],\n",
    "    lora_alpha=config[\"lora\"][\"alpha\"],\n",
    "    lora_dropout=config[\"lora\"][\"dropout\"],\n",
    "    target_modules=config[\"lora\"][\"target_modules\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Enable gradient checkpointing if configured\n",
    "if config[\"training\"][\"gradient_checkpointing\"]:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "# Print trainable parameters info\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Setup\n",
    "\n",
    "Configure the training arguments and data collator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config[\"model\"][\"output_dir\"],\n",
    "    per_device_train_batch_size=config[\"training\"][\"batch_size\"],\n",
    "    per_device_eval_batch_size=config[\"training\"][\"batch_size\"],\n",
    "    gradient_accumulation_steps=config[\"training\"][\"gradient_accumulation_steps\"],\n",
    "    learning_rate=config[\"training\"][\"learning_rate\"],\n",
    "    num_train_epochs=config[\"training\"][\"num_train_epochs\"],\n",
    "    warmup_ratio=config[\"training\"][\"warmup_ratio\"],\n",
    "    weight_decay=config[\"training\"][\"weight_decay\"],\n",
    "    lr_scheduler_type=config[\"training\"][\"lr_scheduler_type\"],\n",
    "    bf16=config[\"training\"][\"bf16\"],\n",
    "    fp16=not config[\"training\"][\"bf16\"],\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=50,\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# Set up data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False  # Causal language modeling, not masked language modeling\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training\n",
    "\n",
    "Now we'll set up the trainer and run the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Run training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model\n",
    "\n",
    "After training, save the model and verify the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the fine-tuned model\n",
    "print(f\"Saving model to {config['model']['output_dir']}\")\n",
    "trainer.save_model()\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(config[\"model\"][\"output_dir\"])\n",
    "\n",
    "# List files in output directory\n",
    "print(\"\\nFiles in output directory:\")\n",
    "for root, dirs, files in os.walk(config[\"model\"][\"output_dir\"]):\n",
    "    level = root.replace(config[\"model\"][\"output_dir\"], '').count(os.sep)\n",
    "    indent = ' ' * 4 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    sub_indent = ' ' * 4 * (level + 1)\n",
    "    for f in files:\n",
    "        print(f\"{sub_indent}{f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Model\n",
    "\n",
    "Let's test our fine-tuned model with a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load fine-tuned model\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the saved LoRA model\n",
    "print(\"Loading fine-tuned model for testing...\")\n",
    "peft_config = PeftConfig.from_pretrained(config[\"model\"][\"output_dir\"])\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    torch_dtype=torch.bfloat16 if config[\"training\"][\"bf16\"] else torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, config[\"model\"][\"output_dir\"])\n",
    "model.eval()\n",
    "\n",
    "# Function for generating text\n",
    "def generate_text(instruction, input_text=\"\"):\n",
    "    # Format the prompt\n",
    "    prompt = PROMPT_TEMPLATE.format(\n",
    "        instruction=instruction,\n",
    "        input=input_text,\n",
    "        output=\"\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode the output\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the generated response (after the prompt)\n",
    "    response = output_text[len(prompt):]\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "# Test with a few examples\n",
    "test_examples = [\n",
    "    {\"instruction\": \"Explain the concept of fine-tuning in machine learning.\", \"input\": \"\"},\n",
    "    {\"instruction\": \"Summarize the following text\", \"input\": \"Llama 3 is Meta's latest text generation AI model family. It's available in two sizes and delivers improvements in multiple dimensions.\"},\n",
    "    # Add more examples specific to your fine-tuning domain\n",
    "]\n",
    "\n",
    "for i, example in enumerate(test_examples):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Instruction: {example['instruction']}\")\n",
    "    if example['input']:\n",
    "        print(f\"Input: {example['input']}\")\n",
    "    print(\"\\nGenerated Response:\")\n",
    "    response = generate_text(example['instruction'], example['input'])\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "Congratulations! You've successfully fine-tuned a Llama 3.3 model using LoRA. Here's a summary of what we've accomplished:\n",
    "\n",
    "1. Set up the environment and configured the training parameters\n",
    "2. Prepared and processed the training dataset\n",
    "3. Set up the model with LoRA for parameter-efficient fine-tuning\n",
    "4. Trained the model on our custom dataset\n",
    "5. Saved and tested the fine-tuned model\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Now that you have a basic understanding of how to fine-tune Llama 3.3 models, you can:\n",
    "\n",
    "1. Experiment with different hyperparameters to improve performance\n",
    "2. Try different LoRA configurations (changing rank, target modules, etc.)\n",
    "3. Expand your training dataset for better results\n",
    "4. Explore more advanced techniques in our other notebooks\n",
    "\n",
    "For more advanced fine-tuning approaches, check out the [memory-efficient fine-tuning notebook](./02_memory_efficient_fine_tuning.ipynb) or explore the [evaluation and testing notebook](./03_evaluation_and_testing.ipynb) to assess your model's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}